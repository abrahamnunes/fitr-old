%
%   The purpose of this experiment is to determine the accuracy of the
%    rlfitkit in identifying the model from which data are produced.
%   Several result sets will be generated by various underlying models.
%   Multiple models will be fit to each set, including one which
%    corresponds to the correct underlying data-generating process.
%   The hypothesis is that the rlfitkit will be able to identify the correct
%    model with higher probability than not.
%
%===============================================================================
%
%   GLOBAL OPTIONS AND PREP
%
%===============================================================================

clear all;

twostepfit            = struct();
twostepfit.nfittrials = 10;

errno = 0;
for k = 1:twostepfit.nfittrials
    experiment           = struct();
    experiment.nsubjects = 50;

    %===========================================================================
    %
    %   CREATE SUBJECTS
    %
    %===========================================================================

    % MIXED MODELS -------------------------------------------------------------
    % group 1
    subjects.name        = '(\alpha, \beta, \omega)';
    subjects.N           = experiment.nsubjects;
    subjects.params      = zeros(subjects.N, 3);
    subjects.params(:,1) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % learning rate
    subjects.params(:,2) = gamrnd(5., 1., [subjects.N, 1]);      % inverse temp
    subjects.params(:,3) = ones(subjects.N, 1);  % eligibility trace
    subjects.params(:,4) = ones(subjects.N, 1);  % reward Sensitiv.
    subjects.params(:,5) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % mb-mf balance

    experiment.groups(1).subjects = subjects; clear subjects;

    % group 2
    subjects.name        = '(\alpha, \beta, \rho, \omega)';
    subjects.N           = experiment.nsubjects;
    subjects.params      = zeros(subjects.N, 3);
    subjects.params(:,1) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % learning rate
    subjects.params(:,2) = gamrnd(5., 1., [subjects.N, 1]);      % inverse temp
    subjects.params(:,3) = ones(subjects.N, 1);  % eligibility trace
    subjects.params(:,4) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % reward Sensitiv.
    subjects.params(:,5) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % mb-mf balance

    experiment.groups(2).subjects = subjects; clear subjects;

    % group 3
    subjects.name        = '(\alpha, \beta, \lambda, \omega)';
    subjects.N           = experiment.nsubjects;
    subjects.params      = zeros(subjects.N, 3);
    subjects.params(:,1) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % learning rate
    subjects.params(:,2) = gamrnd(5., 1., [subjects.N, 1]);      % inverse temp
    subjects.params(:,3) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % eligibility trace
    subjects.params(:,4) = ones(subjects.N, 1);  % reward Sensitiv.
    subjects.params(:,5) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % mb-mf balance

    experiment.groups(3).subjects = subjects; clear subjects;

    % group 4
    subjects.name        = '(\alpha, \beta, \lambda, \rho, \omega)';
    subjects.N           = experiment.nsubjects;
    subjects.params      = zeros(subjects.N, 3);
    subjects.params(:,1) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % learning rate
    subjects.params(:,2) = gamrnd(5., 1., [subjects.N, 1]);      % inverse temp
    subjects.params(:,3) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % eligibility trace
    subjects.params(:,4) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % reward Sensitiv.
    subjects.params(:,5) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % mb-mf balance

    experiment.groups(4).subjects = subjects; clear subjects;

    % MODELFREE MODELS ---------------------------------------------------------
    % group 5
    subjects.name        = 'MF(\alpha, \beta)';
    subjects.N           = experiment.nsubjects;
    subjects.params      = zeros(subjects.N, 3);
    subjects.params(:,1) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % learning rate
    subjects.params(:,2) = gamrnd(5., 1., [subjects.N, 1]);      % inverse temp
    subjects.params(:,3) = ones(subjects.N, 1);  % eligibility trace
    subjects.params(:,4) = ones(subjects.N, 1);  % reward Sensitiv.
    subjects.params(:,5) = zeros(subjects.N, 1);  % mb-mf balance

    experiment.groups(5).subjects = subjects; clear subjects;

    % group 6
    %subjects.name        = 'MF(\alpha, \beta, \rho)';
    %subjects.N           = experiment.nsubjects;
    %subjects.params      = zeros(subjects.N, 3);
    %subjects.params(:,1) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % learning rate
    %subjects.params(:,2) = gamrnd(5., 1., [subjects.N, 1]);      % inverse temp
    %subjects.params(:,3) = ones(subjects.N, 1);  % eligibility trace
    %subjects.params(:,4) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % reward Sensitiv.
    %subjects.params(:,5) = zeros(subjects.N, 1);  % mb-mf balance

    %experiment.groups(6).subjects = subjects; clear subjects;

    % group 7
    %subjects.name        = 'MF(\alpha, \beta, \lambda)';
    %subjects.N           = experiment.nsubjects;
    %subjects.params      = zeros(subjects.N, 3);
    %subjects.params(:,1) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % learning rate
    %subjects.params(:,2) = gamrnd(5., 1., [subjects.N, 1]);      % inverse temp
    %subjects.params(:,3) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % eligibility trace
    %subjects.params(:,4) = ones(subjects.N, 1);  % reward Sensitiv.
    %subjects.params(:,5) = zeros(subjects.N, 1);  % mb-mf balance

    %experiment.groups(7).subjects = subjects; clear subjects;

    % group 8
    %subjects.name        = 'MF(\alpha, \beta, \lambda, \rho)';
    %subjects.N           = experiment.nsubjects;
    %subjects.params      = zeros(subjects.N, 3);
    %subjects.params(:,1) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % learning rate
    %subjects.params(:,2) = gamrnd(5., 1., [subjects.N, 1]);      % inverse temp
    %subjects.params(:,3) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % eligibility trace
    %subjects.params(:,4) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % reward Sensitiv.
    %subjects.params(:,5) = zeros(subjects.N, 1);  % mb-mf balance

    %experiment.groups(8).subjects = subjects; clear subjects;

    % MODELBASED MODELS --------------------------------------------------------
    % group 9
    subjects.name        = 'MB(\alpha, \beta)';
    subjects.N           = experiment.nsubjects;
    subjects.params      = zeros(subjects.N, 3);
    subjects.params(:,1) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % learning rate
    subjects.params(:,2) = gamrnd(5., 1., [subjects.N, 1]);      % inverse temp
    subjects.params(:,3) = ones(subjects.N, 1);  % eligibility trace
    subjects.params(:,4) = ones(subjects.N, 1);  % reward Sensitiv.
    subjects.params(:,5) = ones(subjects.N, 1);  % mb-mf balance

    experiment.groups(6).subjects = subjects; clear subjects;

    % group 10
    %subjects.name        = 'MB(\alpha, \beta, \rho)';
    %subjects.N           = experiment.nsubjects;
    %subjects.params      = zeros(subjects.N, 3);
    %subjects.params(:,1) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % learning rate
    %subjects.params(:,2) = gamrnd(5., 1., [subjects.N, 1]);      % inverse temp
    %subjects.params(:,3) = ones(subjects.N, 1);  % eligibility trace
    %subjects.params(:,4) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % reward Sensitiv.
    %subjects.params(:,5) = ones(subjects.N, 1);  % mb-mf balance

    %experiment.groups(10).subjects = subjects; clear subjects;

    % group 11
    %subjects.name        = 'MB(\alpha, \beta, \lambda)';
    %subjects.N           = experiment.nsubjects;
    %subjects.params      = zeros(subjects.N, 3);
    %subjects.params(:,1) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % learning rate
    %subjects.params(:,2) = gamrnd(5., 1., [subjects.N, 1]);      % inverse temp
    %subjects.params(:,3) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % eligibility trace
    %subjects.params(:,4) = ones(subjects.N, 1);  % reward Sensitiv.
    %subjects.params(:,5) = ones(subjects.N, 1);  % mb-mf balance

    %experiment.groups(11).subjects = subjects; clear subjects;

    % group 12
    %subjects.name        = 'MB(\alpha, \beta, \lambda, \rho)';
    %subjects.N           = experiment.nsubjects;
    %subjects.params      = zeros(subjects.N, 3);
    %subjects.params(:,1) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % learning rate
    %subjects.params(:,2) = gamrnd(5., 1., [subjects.N, 1]);      % inverse temp
    %subjects.params(:,3) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % eligibility trace
    %subjects.params(:,4) = betarnd(1.1, 1.1,  [subjects.N, 1]);  % reward Sensitiv.
    %subjects.params(:,5) = ones(subjects.N, 1);  % mb-mf balance

    %experiment.groups(12).subjects = subjects; clear subjects;

    %===========================================================================
    %
    %   GENERATE DATA
    %
    %===========================================================================

    % Set up task parameters
    experiment.task.ntrials   = 201;
    experiment.task.nstates   = 3;
    experiment.task.nactions  = 2;
    experiment.task.ptrans    = [0.3 0.7];
    experiment.task.pathsigma = 0.25;

    experiment.ngroups = size(experiment.groups, 2);
    for i = 1:experiment.ngroups
        experiment.groups(i).results = twostep.vanilla(experiment.groups(i).subjects, experiment.task);
        disp(['Generated Group ', num2str(i), ' data']);
    end

    disp('----- FINISHED GENERATING DATA -----');

    %===========================================================================
    %
    %   CREATE MODELS
    %
    %===========================================================================

    % MIXED MODELS -------------------------------------------------------------
    experiment.model(1).name     = '(\alpha, \beta, \omega)';
    experiment.model(1).lik      = @twostepll.lrbetaomega;
    experiment.model(1).param(1) = rlparam.learningrate();
    experiment.model(1).param(2) = rlparam.inversetemp();
    experiment.model(1).param(3) = rlparam.mbmfbalance();

    experiment.model(2).name     = '(\alpha, \beta, \rho, \omega)';
    experiment.model(2).lik      = @twostepll.lrbetarhoomega;
    experiment.model(2).param(1) = rlparam.learningrate();
    experiment.model(2).param(2) = rlparam.inversetemp();
    experiment.model(2).param(3) = rlparam.rewardsensitivity();
    experiment.model(2).param(4) = rlparam.mbmfbalance();

    experiment.model(3).name     = '(\alpha, \beta, \lambda, \omega)';
    experiment.model(3).lik      = @twostepll.lrbetalambdaomega;
    experiment.model(3).param(1) = rlparam.learningrate();
    experiment.model(3).param(2) = rlparam.inversetemp();
    experiment.model(3).param(3) = rlparam.etraceparam();
    experiment.model(3).param(4) = rlparam.mbmfbalance();

    experiment.model(4).name     = '(\alpha, \beta, \lambda, \rho, \omega)';
    experiment.model(4).lik      = @twostepll.lrbetalambdarhoomega;
    experiment.model(4).param(1) = rlparam.learningrate();
    experiment.model(4).param(2) = rlparam.inversetemp();
    experiment.model(4).param(3) = rlparam.etraceparam();
    experiment.model(4).param(4) = rlparam.rewardsensitivity();
    experiment.model(4).param(5) = rlparam.mbmfbalance();

    % MODELFREE MODELS ---------------------------------------------------------
    experiment.model(5).name     = 'MF(\alpha, \beta)';
    experiment.model(5).lik      = @twostepll.mf_lrbeta;
    experiment.model(5).param(1) = rlparam.learningrate();
    experiment.model(5).param(2) = rlparam.inversetemp();

    %experiment.model(6).name     = 'MF(\alpha, \beta, \rho)';
    %experiment.model(6).lik      = @twostepll.mf_lrbetarho;
    %experiment.model(6).param(1) = rlparam.learningrate();
    %experiment.model(6).param(2) = rlparam.inversetemp();
    %experiment.model(6).param(3) = rlparam.rewardsensitivity();

    %experiment.model(7).name     = 'MF(\alpha, \beta, \lambda)';
    %experiment.model(7).lik      = @twostepll.mf_lrbetalambda;
    %experiment.model(7).param(1) = rlparam.learningrate();
    %experiment.model(7).param(2) = rlparam.inversetemp();
    %experiment.model(7).param(3) = rlparam.etraceparam();

    %experiment.model(8).name     = 'MF(\alpha, \beta, \lambda, \rho)';
    %experiment.model(8).lik      = @twostepll.mf_lrbetalambdarho;
    %experiment.model(8).param(1) = rlparam.learningrate();
    %experiment.model(8).param(2) = rlparam.inversetemp();
    %experiment.model(8).param(3) = rlparam.etraceparam();
    %experiment.model(8).param(4) = rlparam.rewardsensitivity();

    % MODELBASED MODELS --------------------------------------------------------
    experiment.model(6).name     = 'MB(\alpha, \beta)';
    experiment.model(6).lik      = @twostepll.mb_lrbeta;
    experiment.model(6).param(1) = rlparam.learningrate();
    experiment.model(6).param(2) = rlparam.inversetemp();

    %experiment.model(10).name     = 'MB(\alpha, \beta, \rho)';
    %experiment.model(10).lik      = @twostepll.mb_lrbetarho;
    %experiment.model(10).param(1) = rlparam.learningrate();
    %experiment.model(10).param(2) = rlparam.inversetemp();
    %experiment.model(10).param(3) = rlparam.rewardsensitivity();

    %experiment.model(11).name     = 'MB(\alpha, \beta, \lambda)';
    %experiment.model(11).lik      = @twostepll.mb_lrbetalambda;
    %experiment.model(11).param(1) = rlparam.learningrate();
    %experiment.model(11).param(2) = rlparam.inversetemp();
    %experiment.model(11).param(3) = rlparam.etraceparam();

    %experiment.model(12).name     = 'MB(\alpha, \beta, \lambda, \rho)';
    %experiment.model(12).lik      = @twostepll.mb_lrbetalambdarho;
    %experiment.model(12).param(1) = rlparam.learningrate();
    %experiment.model(12).param(2) = rlparam.inversetemp();
    %experiment.model(12).param(3) = rlparam.etraceparam();
    %experiment.model(12).param(4) = rlparam.rewardsensitivity();

    % RANDOM MODEL -------------------------------------------------------------
    experiment.model(7).name     = 'Random';
    experiment.model(7).lik      = @twostepll.randmodel;
    experiment.model(7).param    = rlparam.inversetemp();

    %===========================================================================
    %
    %   FIT AND COMPARE MODELS
    %
    %===========================================================================

    experiment.fitoptions.maxiters   = 1000;
    experiment.fitoptions.nstarts    = 5;
    experiment.fitoptions.climit     = 10;

    experiment.nmodels = size(experiment.model, 2);

    for i = 1:experiment.ngroups
            for j = 1:experiment.nmodels
                try
                    experiment.groups(i).rlfits(j).name = experiment.model(j).name;
                    experiment.groups(i).rlfits(j).fit = fitmodel(experiment.groups(i).results, experiment.model(j), experiment.fitoptions);

                    disp(['----- COMPLETED FITTING MODEL ', num2str(j), ' TO GROUP ', num2str(i), ' ----- ']);
                catch
                    warning(['Fitting model ', num2str(j), ' to Group ', num2str(i), ' failed.']);
                    errno = errno + 1;
                    twostepfit.errmsg{errno} = ['Run ', num2str(k), 'Fitting model ', num2str(j), ' to Group ', num2str(i), ' failed.'];
                end
            end

			try
        		experiment.groups(i).bms = BMS(experiment.groups(i).rlfits);
        		disp(['----- COMPLETED BAYESIAN MODEL SELECTION FOR GROUP ', num2str(i), ' -----']);
        	catch
        		warning(['BMS Group ', num2str(i), ' failed.']);
        		errno = errno + 1;
        		twostepfit.errmsg{errno} = ['BMS Group ', num2str(i), ' failed.'];
        	end
    end

    % Store in the overall structure
    twostepfit.trial(k).experiment = experiment;
    clear experiment;
end

clear errno;
clear i; clear j; clear k;
%===============================================================================
%
%   PLOT THE TASK OUTCOMES
%
%===============================================================================

% Cumulative rewards
%csr = cumsum(reshape([results.R], [subjects.N, taskparams.ntrials])');
%figure();
%g = pqts(csr, [0, taskparams.ntrials], 0.3, {'seriesmean'});
%xlabel('Trial');
%ylabel('Cumulative Reward');
%pqtitle(g, 'Cumulative Rewards');

% Reward Prediction Errors
%rpes = abs(reshape([results.rpe], [subjects.N, taskparams.ntrials])');
%figure();
%g = pqts(rpes, [0, taskparams.ntrials], 0.1, {'seriesmean'});
%xlabel('Trial');
%ylabel('Reward Prediction Error');
%pqtitle(g, 'Reward Prediction Error');

%===============================================================================
%
%   PLOT MODEL FITTING RESULTS
%
%===============================================================================

% Parameter scatterplots
%figure();
%subplot(3, 1, 1);
%g1 = pqscatter(subjects.params(:,1), horzcat(fit1.params(:,1),    fit2.params(:,1), fit3.params(:,1), fit4.params(:,1), fit5.params(:,1), fit6.params(:,1)), {'match'});
%xlabel('Actual');
%ylabel('Estimate');
%pqtitle(g1, 'Learning Rate (\alpha)');

%subplot(3, 1, 2);
%g2 = pqscatter(subjects.params(:,2), horzcat(fit1.params(:,2),    fit2.params(:,2), fit3.params(:,2), fit4.params(:,2), fit5.params(:,2), fit6.params(:,2)), {'match'});
%xlabel('Actual');
%ylabel('Estimate');
%pqtitle(g2, 'Inverse Temperature (\beta)');

%subplot(3, 1, 3);
%g3 = pqscatter(subjects.params(:,3), horzcat(fit1.params(:,3),    fit2.params(:,4), fit3.params(:,3), fit4.params(:,4), fit5.params(:,3), fit6.params(:,4)), {'match'});
%xlabel('Actual');
%ylabel('Estimate');
%pqtitle(g3, 'MB/MF Balance (\omega)');

% Plot correlograms
%figure();
%g1 = pqcorrelogram(horzcat(subjects.params(:, 1), fit1.params(:,1),    fit2.params(:,1), fit3.params(:,1), fit4.params(:,1), fit5.params(:,1), fit6.params(:,1)), {'Truth', 'Model 1', 'Model 2', 'Model 3', 'Model 4', 'Model 5', 'Model 6'});
%pqtitle(g1, '\alpha Correlations');

%figure();
%g2 = pqcorrelogram(horzcat(subjects.params(:, 2), fit1.params(:,2),    fit2.params(:,2), fit3.params(:,2), fit4.params(:,2), fit5.params(:,2), fit6.params(:,2)), {'Truth', 'Model 1', 'Model 2', 'Model 3', 'Model 4', 'Model 5', 'Model 6'});
%pqtitle(g2, '\beta Correlations');

%figure();
%g3 = pqcorrelogram(horzcat(subjects.params(:, 3), fit1.params(:,3),    fit2.params(:,4), fit3.params(:,3), fit4.params(:,4), fit5.params(:,3), fit6.params(:,4)), {'Truth', 'Model 1', 'Model 2', 'Model 3', 'Model 4', 'Model 5', 'Model 6'});
%pqtitle(g3, '\omega Correlations');

% Plot fit results
%figure();
%subplot(3, 2, 1);
%g1 = pqbar({'{\alpha, \beta, \omega}', '{\alpha, \beta, \lambda, \omega}', 'Pure MB', 'Pure MB \lambda', 'Pure MF', 'Pure MF \lambda', 'Rand'}, [sum(fit1.sAIC) sum(fit2.sAIC) sum(fit3.sAIC) sum(fit4.sAIC) sum(fit5.sAIC) sum(fit6.sAIC) sum(fit7.sAIC)]);
%pqtitle(g1, 'Aikake Information Criterion');

%subplot(3, 2, 2);
%g2 = pqbar({'{\alpha, \beta, \omega}', '{\alpha, \beta, \lambda, \omega}', 'Pure MB', 'Pure MB \lambda', 'Pure MF', 'Pure MF \lambda', 'Rand'}, [sum(fit1.sBIC) sum(fit2.sBIC) sum(fit3.sBIC) sum(fit4.sBIC) sum(fit5.sBIC) sum(fit6.sBIC) sum(fit7.sBIC)]);
%pqtitle(g2, 'Bayesian Information Criterion');

%subplot(3, 2, 3);
%g3 = pqbar({'{\alpha, \beta, \omega}', '{\alpha, \beta, \lambda, \omega}', 'Pure MB', 'Pure MB \lambda', 'Pure MF', 'Pure MF \lambda', 'Rand'}, bms.xp);
%pqtitle(g3, 'Exceedance Probabilities');

%subplot(3, 2, 4);
%g4 = pqbar({'{\alpha, \beta, \omega}', '{\alpha, \beta, \lambda, \omega}', 'Pure MB', 'Pure MB \lambda', 'Pure MF', 'Pure MF \lambda', 'Rand'}, bms.pxp);
%pqtitle(g4, 'Protected Exceedance Probabilities');

%subplot(3, 2, 5);
%g5 = pqbar({'{\alpha, \beta, \omega}', '{\alpha, \beta, \lambda, \omega}', 'Pure MB', 'Pure MB \lambda', 'Pure MF', 'Pure MF \lambda', 'Rand'}, [exp(log(fit1.pp)/2), exp(log(fit2.pp)/2), exp(log(fit3.pp)/2), exp(log(fit4.pp)/2), exp(log(fit5.pp)/2), exp(log(fit6.pp)/2), exp(log(fit7.pp)/2)]);
%pqtitle(g5, 'Total Predictive Probability');
